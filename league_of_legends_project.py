# -*- coding: utf-8 -*-
"""league_of_legends Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A5-nuN3XQmge-PKih9Jp7AmrcENdMj--
"""

!pip install ydata-profiling

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import math
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from xgboost import XGBClassifier
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, r2_score, mean_absolute_error, mean_squared_error

import warnings
warnings.filterwarnings('ignore')
from ydata_profiling import ProfileReport

pip freeze > requirements.txt

used_libs = [
    "pandas",
    "numpy",
    "matplotlib",
    "seaborn",
    "plotly",
    "scikit-learn",
    "xgboost",
    "tensorflow",
    "ydata-profiling"
]


with open("/content/requirements.txt", "r") as f:
    lines = f.readlines()


filtered = []
for line in lines:
    for lib in used_libs:
        if line.lower().startswith(lib.lower() + "=="):
            filtered.append(line.strip())
            break

with open("/content/requirements.txt", "w") as f:
    f.write("\n".join(filtered))

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

df = pd.read_csv("hf://datasets/BoostedJonP/league_of_legends_match_data/league_of_legends_emerald_match_data.csv")

df.head()

df.info(verbose=True, show_counts=True)

df.describe()

"""# **Data preprocessing**"""

df.duplicated().sum()

df = df.drop_duplicates()

id_cols = []
for col in df.columns:
  if 'id' in col:
    id_cols.append(col)
    print(f'{col}: {df[col].nunique()} unique values')

df['champion_name'].nunique()

cols_to_drop = ['match_id', 'champion_name', 'champion_skin_id', 'summoner_1_id', 'summoner_2_id']
df = df.drop(cols_to_drop, axis=1)

cols_to_drop = []
for col in df.columns:
  if df[col].nunique() == 1:
    cols_to_drop.append(col)
    print(col)

df = df.drop(cols_to_drop, axis=1)

df.isna().sum()[df.isna().sum() > 0]

df['team_position'] = df.groupby('champion_id')['team_position'].transform(lambda x: x.fillna(x.mode()[0]))

df.isna().sum()[df.isna().sum() > 0]

for col in df.columns:
  if df[col].isna().sum() > 0:
    df[col] = df[col].fillna(-1)

df.isna().sum()[df.isna().sum() > 0]

ban_cols = ['ban_1_champion_id', 'ban_2_champion_id', 'ban_3_champion_id', 'ban_4_champion_id', 'ban_5_champion_id']
df['num_of_bans'] = df[ban_cols].apply(lambda row: (row != -1).sum(), axis=1)

df = df.drop(ban_cols, axis=1)

num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()

rows = math.ceil(len(num_cols) / 3)

fig = make_subplots(
    rows=rows,
    cols=3,
    subplot_titles=num_cols
)

for i, col in enumerate(num_cols, 1):
    row = math.ceil(i / 3)
    col_pos = (i - 1) % 3 + 1
    fig.add_trace(
        go.Box(y=df[col], name=col),
        row=row, col=col_pos
    )

fig.update_layout(
    height=350 * rows,
    title_text="Numerical Variables Boxplots",
    showlegend=False
)

fig.show()

rows = math.ceil(len(num_cols) / 3)

fig = make_subplots(
    rows=rows,
    cols=3,
    subplot_titles=num_cols
)

for i, col in enumerate(num_cols, 1):
    row = math.ceil(i / 3)
    col_pos = (i - 1) % 3 + 1
    fig.add_trace(
        go.Histogram(x=df[col], name=col),
        row=row, col=col_pos
    )

fig.update_layout(
    height=350 * rows,
    title_text="Numerical Variables Histograms",
    showlegend=False
)

fig.show()

scaler = RobustScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

obj_cols = df.select_dtypes(include=['object'])
obj_cols.columns.to_list()

for col in obj_cols.columns:
  print(f'{col}\n{df[col].unique()}')

encoder = LabelEncoder()
for col in obj_cols.columns:
  df[col] = encoder.fit_transform(df[col])

bool_cols = df.select_dtypes(include=['bool'])
bool_cols.columns.to_list()

cleaned_df = df.copy()

weak_corr = cleaned_df.corr()['win'].abs().sort_values(ascending=False).where(lambda x: x < 0.05).dropna().index
weak_corr

cleaned_df = cleaned_df.drop(weak_corr, axis=1)

plt.figure(figsize=(34, 34))
sns.heatmap(cleaned_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

cleaned_df["objective_takedowns"] = cleaned_df["turret_takedowns"] + cleaned_df["inhibitor_takedowns"] + cleaned_df["nexus_takedowns"]
cleaned_df.drop(["turret_takedowns","inhibitor_takedowns","nexus_takedowns"], axis=1, inplace=True)

cleaned_df["epic_monster_kills"] = cleaned_df["dragon_kills"] + cleaned_df["baron_kills"]
cleaned_df.drop(["dragon_kills","baron_kills"], axis=1, inplace=True)

cleaned_df["damage_ratio"] = cleaned_df["total_damage_dealt_to_champions"] / (cleaned_df["total_damage_taken"]+1)

drop_corr = [
    "champion_experience",
    "gold_spent",
    "total_damage_taken",
    "total_damage_dealt_to_champions",
    "true_damage_dealt",
]

cleaned_df = cleaned_df.drop(drop_corr, axis=1)

plt.figure(figsize=(30, 30))
sns.heatmap(cleaned_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

sns.countplot(x='win', data=cleaned_df)
plt.title('Win/Loss Distribution')
plt.show()

profile = ProfileReport(cleaned_df, title="Pandas Profiling Report", explorative=True)
profile

"""# **Binary Classification**"""

X = cleaned_df.drop('win', axis=1)
y = cleaned_df['win']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

accuracy_scores = {}
precision_scores = {}
recall_scores = {}
f1_scores = {}

lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
accuracy_scores['Logistic Regression'] = accuracy_score(y_test, y_pred)
precision_scores['Logistic Regression'] = precision_score(y_test, y_pred, average='weighted')
recall_scores['Logistic Regression'] = recall_score(y_test, y_pred, average='weighted')
f1_scores['Logistic Regression'] = f1_score(y_test, y_pred, average='weighted')
print(classification_report(y_test, y_pred))

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
accuracy_scores['KNN'] = accuracy_score(y_test, y_pred)
precision_scores['KNN'] = precision_score(y_test, y_pred, average='weighted')
recall_scores['KNN'] = recall_score(y_test, y_pred, average='weighted')
f1_scores['KNN'] = f1_score(y_test, y_pred, average='weighted')
print(classification_report(y_test, y_pred))

param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None] + list(range(1, 11)),
    'min_samples_split': [2, 3, 4, 5],
    'min_samples_leaf': [1, 2, 3, 4]
}

dt = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_dt = grid_search.best_estimator_
print(best_params)

y_pred = best_dt.predict(X_test)
accuracy_scores['Decision Tree'] = accuracy_score(y_test, y_pred)
precision_scores['Decision Tree'] = precision_score(y_test, y_pred, average='weighted')
recall_scores['Decision Tree'] = recall_score(y_test, y_pred, average='weighted')
f1_scores['Decision Tree'] = f1_score(y_test, y_pred, average='weighted')
print(classification_report(y_test, y_pred))

rf = RandomForestClassifier(random_state=42, n_estimators=200)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
accuracy_scores['Random Forest'] = accuracy_score(y_test, y_pred)
precision_scores['Random Forest'] = precision_score(y_test, y_pred, average='weighted')
recall_scores['Random Forest'] = recall_score(y_test, y_pred, average='weighted')
f1_scores['Random Forest'] = f1_score(y_test, y_pred, average='weighted')
print(classification_report(y_test, y_pred))

importances = rf.feature_importances_
feat_importances = pd.Series(importances, index=X.columns)
feat_importances.nlargest(15).plot(kind='barh')
plt.title("Top 15 Important Features (RandomForest)")
plt.show()

svc = SVC(random_state=42, kernel='rbf', gamma='scale', C=15)
svc.fit(X_train, y_train)
y_pred = svc.predict(X_test)
accuracy_scores['SVC'] = accuracy_score(y_test, y_pred)
precision_scores['SVC'] = precision_score(y_test, y_pred, average='weighted')
recall_scores['SVC'] = recall_score(y_test, y_pred, average='weighted')
f1_scores['SVC'] = f1_score(y_test, y_pred, average='weighted')
print(classification_report(y_test, y_pred))

xgb = XGBClassifier(random_state=42, n_estimators=200, max_depth=None, learning_rate=0.2)
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)
accuracy_scores['XGBoost'] = accuracy_score(y_test, y_pred)
precision_scores['XGBoost'] = precision_score(y_test, y_pred, average='weighted')
recall_scores['XGBoost'] = recall_score(y_test, y_pred, average='weighted')
f1_scores['XGBoost'] = f1_score(y_test, y_pred, average='weighted')
print(classification_report(y_test, y_pred))

nn = Sequential([
    Dense(512, activation='relu', input_shape=(X_train.shape[1:])),
    Dense(256, activation='relu'),
    Dropout(0.2),
    Dense(256, activation='relu'),
    Dropout(0.2),
    Dense(128, activation='relu'),
    Dense(1, activation='sigmoid')
])

nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
callback = EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True)

history = nn.fit(X_train, y_train, epochs=100, batch_size=32, callbacks=callback, validation_split=0.2)

nn_loss, nn_accuracy = nn.evaluate(X_test, y_test)
print(f'Test Loss: {nn_loss}, Test Accuracy: {nn_accuracy}')
accuracy_scores['Neural Network'] = nn_accuracy
y_pred_nn = np.argmax(nn.predict(X_test), axis=1)
precision_scores['Neural Network'] = precision_score(y_test, y_pred_nn, average='weighted')
recall_scores['Neural Network'] = recall_score(y_test, y_pred_nn, average='weighted')
f1_scores['Neural Network'] = f1_score(y_test, y_pred_nn, average='weighted')

metrics_df = pd.DataFrame({
    'Model': list(accuracy_scores.keys()),
    'Accuracy': list(accuracy_scores.values()),
    'Precision': list(precision_scores.values()),
    'Recall': list(recall_scores.values()),
    'F1-score': list(f1_scores.values())
})

metrics_df_melted = metrics_df.melt(id_vars='Model', var_name='Metric', value_name='Score')

plt.figure(figsize=(12, 7))
ax = sns.barplot(x='Model', y='Score', hue='Metric', data=metrics_df_melted, palette='Set1')
plt.title('Model Performance Comparison (Accuracy, Precision, Recall, and F1-score)')
plt.ylabel('Score')
plt.xlabel('')
plt.ylim(0, 1)
plt.xticks(rotation=45, ha='right')

for container in ax.containers:
    ax.bar_label(container, fmt='%.2f', label_type='edge', padding=2, fontsize=8)

sns.despine(top=True, right=True)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

plt.tight_layout()
plt.show()

"""# **Regression**"""

cleaned_df = df.copy()

weak_corr = cleaned_df.corr()['gold_earned'].abs().sort_values(ascending=False).where(lambda x: x < 0.05).dropna().index
weak_corr

cleaned_df = cleaned_df.drop(weak_corr, axis=1)

cleaned_df["objective_takedowns"] = cleaned_df["turret_takedowns"] + cleaned_df["inhibitor_takedowns"] + cleaned_df["nexus_takedowns"]
cleaned_df.drop(["turret_takedowns","inhibitor_takedowns","nexus_takedowns"], axis=1, inplace=True)

cleaned_df["epic_monster_kills"] = cleaned_df["dragon_kills"] + cleaned_df["baron_kills"]
cleaned_df.drop(["dragon_kills","baron_kills"], axis=1, inplace=True)

cleaned_df["damage_ratio"] = cleaned_df["total_damage_dealt_to_champions"] / (cleaned_df["total_damage_taken"]+1)

X = cleaned_df.drop('gold_earned', axis=1)
y = cleaned_df['gold_earned']

pca = PCA(n_components=20)
X_pca = pca.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

r2_scores = {}
mae_scores = {}
mse_scores = {}

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_pred = lin_reg.predict(X_test)
r2_scores['Linear Regression'] = r2_score(y_test, y_pred)
mae_scores['Linear Regression'] = mean_absolute_error(y_test, y_pred)
mse_scores['Linear Regression'] = mean_squared_error(y_test, y_pred)
print(f'R2 Score: {r2_score(y_test, y_pred)}')
print(f'Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}')
print(f'Mean Squared Error: {mean_squared_error(y_test, y_pred)}')

linear_nn = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1:])),
    Dense(256, activation='relu'),
    Dense(512, activation='relu'),
    Dense(128, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(1, activation='linear')
])
linear_nn.compile(optimizer='adam', loss='mean_squared_error', metrics=['r2_score'])
callback = EarlyStopping(monitor='val_r2_score', patience=10, mode='max', restore_best_weights=True)

history = linear_nn.fit(X_train, y_train, epochs=100, batch_size=32, callbacks=callback, validation_split=0.2)

linear_nn_loss, linear_nn_r2 = linear_nn.evaluate(X_test, y_test)
print(f'Test Loss: {linear_nn_loss}, Test R2 Score: {linear_nn_r2}')
r2_scores['Neural Network'] = linear_nn_r2
mae_scores['Neural Network'] = mean_absolute_error(y_test, linear_nn.predict(X_test))
mse_scores['Neural Network'] = mean_squared_error(y_test, linear_nn.predict(X_test))

metrics_df_reg = pd.DataFrame({
    'Model': list(r2_scores.keys()),
    'R2 Score': list(r2_scores.values()),
    'MAE': list(mae_scores.values()),
    'MSE': list(mse_scores.values())
})

metrics_df_reg_melted = metrics_df_reg.melt(id_vars='Model', var_name='Metric', value_name='Score')

plt.figure(figsize=(12, 7))
ax = sns.barplot(x='Model', y='Score', hue='Metric', data=metrics_df_reg_melted, palette='Set1')
plt.title('Regression Model Performance Comparison (R2 Score, MAE, and MSE)')
plt.ylabel('Score')
plt.xlabel('')
plt.ylim(metrics_df_reg_melted['Score'].min() - 0.1, metrics_df_reg_melted['Score'].max() + 0.1)
plt.xticks(rotation=45, ha='right')

for container in ax.containers:
    ax.bar_label(container, fmt='%.2f', label_type='edge', padding=2, fontsize=8)

sns.despine(top=True, right=True)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

plt.tight_layout()
plt.show()

cleaned_df = df.copy()

cleaned_df["objective_takedowns"] = cleaned_df["turret_takedowns"] + cleaned_df["inhibitor_takedowns"] + cleaned_df["nexus_takedowns"]
cleaned_df.drop(["turret_takedowns","inhibitor_takedowns","nexus_takedowns"], axis=1, inplace=True)

cleaned_df["epic_monster_kills"] = cleaned_df["dragon_kills"] + cleaned_df["baron_kills"]
cleaned_df.drop(["dragon_kills","baron_kills"], axis=1, inplace=True)

cleaned_df["damage_ratio"] = cleaned_df["total_damage_dealt_to_champions"] / (cleaned_df["total_damage_taken"]+1)

"""# **Multi Classification**"""

weak_corr = cleaned_df.corr()['team_position'].abs().sort_values(ascending=False).where(lambda x: x < 0.05).dropna().index
weak_corr

cleaned_df = cleaned_df.drop(weak_corr, axis=1)

X = cleaned_df.drop('team_position', axis=1)
y = cleaned_df['team_position']

pca = PCA(n_components=20)
X_pca = pca.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

accuracy_scores = {}
precision_scores = {}
recall_scores = {}
f1_scores = {}

lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
accuracy_scores['Logistic Regression'] = accuracy_score(y_test, y_pred)
precision_scores['Logistic Regression'] = precision_score(y_test, y_pred, average='weighted')
recall_scores['Logistic Regression'] = recall_score(y_test, y_pred, average='weighted')
f1_scores['Logistic Regression'] = f1_score(y_test, y_pred, average='weighted')
print(classification_report(y_test, y_pred))

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
accuracy_scores['KNN'] = accuracy_score(y_test, y_pred)
precision_scores['KNN'] = precision_score(y_test, y_pred, average='weighted')
recall_scores['KNN'] = recall_score(y_test, y_pred, average='weighted')
f1_scores['KNN'] = f1_score(y_test, y_pred, average='weighted')
print(classification_report(y_test, y_pred))

param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None] + list(range(1, 11)),
    'min_samples_split': [2, 3, 4, 5],
    'min_samples_leaf': [1, 2, 3, 4]
}

dt = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_dt = grid_search.best_estimator_
print(best_params)

y_pred = best_dt.predict(X_test)
accuracy_scores['Decision Tree'] = accuracy_score(y_test, y_pred)
precision_scores['Decision Tree'] = precision_score(y_test, y_pred, average='weighted')
recall_scores['Decision Tree'] = recall_score(y_test, y_pred, average='weighted')
f1_scores['Decision Tree'] = f1_score(y_test, y_pred, average='weighted')
print(classification_report(y_test, y_pred))

rf = RandomForestClassifier(random_state=42, n_estimators=200)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
accuracy_scores['Random Forest'] = accuracy_score(y_test, y_pred)
precision_scores['Random Forest'] = precision_score(y_test, y_pred, average='weighted')
recall_scores['Random Forest'] = recall_score(y_test, y_pred, average='weighted')
f1_scores['Random Forest'] = f1_score(y_test, y_pred, average='weighted')
print(classification_report(y_test, y_pred))

svc = SVC(random_state=42, kernel='rbf', gamma=0.005, C=20)
svc.fit(X_train, y_train)
y_pred = svc.predict(X_test)
accuracy_scores['SVC'] = accuracy_score(y_test, y_pred)
precision_scores['SVC'] = precision_score(y_test, y_pred, average='weighted')
recall_scores['SVC'] = recall_score(y_test, y_pred, average='weighted')
f1_scores['SVC'] = f1_score(y_test, y_pred, average='weighted')
print(classification_report(y_test, y_pred))

xgb = XGBClassifier(random_state=42, n_estimators=200, max_depth=None, learning_rate=0.2)
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)
accuracy_scores['XGBoost'] = accuracy_score(y_test, y_pred)
precision_scores['XGBoost'] = precision_score(y_test, y_pred, average='weighted')
recall_scores['XGBoost'] = recall_score(y_test, y_pred, average='weighted')
f1_scores['XGBoost'] = f1_score(y_test, y_pred, average='weighted')
print(classification_report(y_test, y_pred))

num_classes = len(np.unique(y_train))
nn = Sequential([
    Dense(512, activation='relu', input_shape=(X_train.shape[1:])),
    Dense(512, activation='relu'),
    Dropout(0.2),
    Dense(256, activation='relu'),
    Dropout(0.2),
    Dense(128, activation='relu'),
    Dense(num_classes, activation='softmax')
])

nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
callback = EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True)

history = nn.fit(X_train, y_train, epochs=100, batch_size=32, callbacks=callback, validation_split=0.2)

nn_loss, nn_accuracy = nn.evaluate(X_test, y_test)
print(f'Test Loss: {nn_loss}, Test Accuracy: {nn_accuracy}')
accuracy_scores['Neural Network'] = nn_accuracy
y_pred_nn = np.argmax(nn.predict(X_test), axis=1)
precision_scores['Neural Network'] = precision_score(y_test, y_pred_nn, average='weighted')
recall_scores['Neural Network'] = recall_score(y_test, y_pred_nn, average='weighted')
f1_scores['Neural Network'] = f1_score(y_test, y_pred_nn, average='weighted')

metrics_df = pd.DataFrame({
    'Model': list(accuracy_scores.keys()),
    'Accuracy': list(accuracy_scores.values()),
    'Precision': list(precision_scores.values()),
    'Recall': list(recall_scores.values()),
    'F1-score': list(f1_scores.values())
})

metrics_df_melted = metrics_df.melt(id_vars='Model', var_name='Metric', value_name='Score')

plt.figure(figsize=(12, 7))
ax = sns.barplot(x='Model', y='Score', hue='Metric', data=metrics_df_melted, palette='Set1')
plt.title('Model Performance Comparison (Accuracy, Precision, Recall, and F1-score)')
plt.ylabel('Score')
plt.xlabel('')
plt.ylim(0, 1)
plt.xticks(rotation=45, ha='right')

for container in ax.containers:
    ax.bar_label(container, fmt='%.2f', label_type='edge', padding=2, fontsize=8)

sns.despine(top=True, right=True)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

plt.tight_layout()
plt.show()